# Authors: Code generated by Google Gemini and comments done by Daniel Holgate
# Date: 19/08/2025
# Description: Code for prompting LLaMA to extract reusable themes from a set of base stories.

import re
from datetime import datetime
import os
import subprocess

from decouple import config

SG_PATH = config("SG_PATH")

# For specifying where your model is located as well as the location of llama-cli which handles running the model for you.
# Must change them for your setup.
LLAMA_CLI_PATH = "/scratch/hlgdan001/llama.cpp/build/bin/llama-cli"
MODEL_PATH = "/scratch/hlgdan001/llama.cpp/hf_models/LLaMA-3.2-3B-Instruct-Q4_K_M.gguf"

MAX_CONTEXT_TOKENS = 8192 # Specifies the total number of tokens that the model can process at once.
GENERATION_BUFFER = 4000 # Specifies the limit of tokens that can be generated by the model in response to a prompt. 

STORY_DELIMITER = "\n\n"

# Files for input and output. Need to be adjusted for the files you are using. 
# The input is the source stories dataset you want to use and the output can be what you want.
INPUT_FILE_PATH = f"{SG_PATH}/filtered_stories_800_Glot.txt"
OUTPUT_FILE_PATH = f"{SG_PATH}/extracted_themes_Glot800.txt"

MAX_STORIES_PER_BATCH = 5

# Method for estimating the number of tokens in a text. This will vary in different situations so just an estimate based on the 
# common conversion between words and tokens.
def simple_token_estimate(text):
    """Estimates tokens based on word count, for batching purposes."""
    return len(text.split()) * 1.3

# The main function for processing and extracting reusable story themes. 
# Takes in an input file, performs what is requested in the prompt and outputs a theme word and sentence for each story in the dataset.
def process_stories_with_llama_cli(
    input_file: str,
    output_file: str,
    llama_cli_path: str,
    model_path: str,
    story_delimiter: str,
    max_context_tokens: int,
    generation_buffer: int
):
    # """
    # Reads stories from an input file, batches them, processes with llama.cpp CLI,
    # and appends themes to an output file.
    # """
    # print(f"Starting processing with llama.cpp CLI at {datetime.now()}")
    # print(f"Input file: {input_file}")
    # print(f"Output file: {output_file}")
    # print(f"llama-cli path: {llama_cli_path}")
    # print(f"Model path: {model_path}")
    # print(f"Max context tokens for model (-c): {max_context_tokens}")
    # print(f"Max generation tokens per batch (-n): {generation_buffer}")

    # Attempts to read in the story dataset for processing.
    try:
        with open(input_file, "r", encoding="utf-8") as f:
            full_text = f.read()
    except FileNotFoundError:
        print(f"Error: Input file '{input_file}' not found.")
        return
    except Exception as e:
        print(f"Error reading input file: {e}")
        return

    # Splits read text into separate stories (are separated by double empty line in the datasets)
    stories_raw = re.split(r'\n\s*\n', full_text.strip(), flags=re.MULTILINE)
    stories = [story.strip() for story in stories_raw if story.strip()]

    if not stories:
        print("No stories found in the input file. Check delimiter and content.")
        return

    print(f"Found {len(stories)} stories in the file.")

    # ADDED SECTION TO PRINT AND VERIFY STORY EXTRACTION
    # print("\n--- Verifying Story Extraction ---")
    # for i, story_content in enumerate(stories):
    #     print(f"Story {i + 1} (first 100 chars):")
    #     # Using chr(10) for newline character to avoid f-string SyntaxError
    #     print(f"    '{story_content[:100].replace(chr(10), ' ')}...'")
    # print("--- End Story Verification ---\n")
    # END OF ADDED SECTION

    processed_themes = {}
    current_batch_stories_data = []
    current_batch_token_count = 0

    # Base prompt to try keep the structure of generation constant.
    BASE_STORY_PROMPT_STRUCTURE = (
        "Story ID: {identifier}\n"
        "Story Text:\n{story_content}\n"
        "---\n"
        "Theme Sentence for {identifier}:\n"
        "Theme Word for {identifier}:\n"
    )

    # Provides the prompt to the model for instructing it in how to extract the reusable themes. Provides examples and instructions.
    # Specifies age to try lead the model to themes appropriate for young children's stories.
    overall_instruction_prefix = (
        """
        I want to extract themes from a set of stories.
        
        For each story in the list following, extract the main theme/idea as a short sentence and then as a single word.
        
        Example of theme extraction (use this style):
        Story ID: Example_Story_1\n
        "Theme Sentence: A lesson in the dangers of overestimating oneself and the importance of humility.\n"
        "Theme Word: Humility\n"
        "---\n\n" # Separator between example and actual stories
        
        These events should be applicable to stories for pupils of age 6.
        
        "**BEGIN STORIES TO ANALYZE**\n\n"
        """
    )
    overall_instruction_suffix = "\n\n**END STORIES TO ANALYZE**"

    # Calculates tokens in the prompt.
    overall_instruction_overhead = simple_token_estimate(
        overall_instruction_prefix + overall_instruction_suffix
    )

    # Calculates number of tokens available for passing input stories to the model.
    MAX_TOKENS_FOR_STORIES = max_context_tokens - overall_instruction_overhead - generation_buffer
    
    # If there are not enough tokens left then raises an error.
    if MAX_TOKENS_FOR_STORIES <= 0:
        print(f"Error: Max context tokens ({max_context_tokens}) too small for overall instruction overhead ({overall_instruction_overhead}) and generation buffer ({generation_buffer}).")
        print("Adjust MAX_CONTEXT_TOKENS or GENERATION_BUFFER, or simplify prompt.")
        return

    story_index = 0
    
    # Loops through the stories 1 by 1.
    while story_index < len(stories):
        story_content = stories[story_index]
        story_identifier = f"Story_{story_index + 1}"

        simulated_story_prompt_part = BASE_STORY_PROMPT_STRUCTURE.format(
            identifier=story_identifier, story_content=story_content
        )
        story_tokens = simple_token_estimate(simulated_story_prompt_part)

        # If tokens in the current batch with the tokens for the current story are within the limits or is the first story and
        # its tokens are outside the limits, adds the story to the processing batch.
        if ((current_batch_token_count + story_tokens <= MAX_TOKENS_FOR_STORIES) and
        (len(current_batch_stories_data) < MAX_STORIES_PER_BATCH)) or not current_batch_stories_data:

            current_batch_stories_data.append((story_identifier, story_content))
            current_batch_token_count += story_tokens
            story_index += 1
            
        # If the batch tokens reach the limit before all stories are in the batch, it calls processes the batch.
        # Then resets the required variables and continues.
        else:
            print(f"\nProcessing batch with {len(current_batch_stories_data)} stories (approx. {current_batch_token_count} content tokens)...")
            batch_prompt_parts = []
            identifiers_in_batch = []
            # Adds the story id and the associated story content to the batch.
            for id_val, content_val in current_batch_stories_data:
                batch_prompt_parts.append(
                    BASE_STORY_PROMPT_STRUCTURE.format(
                        identifier=id_val, story_content=content_val
                    )
                )
                identifiers_in_batch.append(id_val)

            # Forms the batch prompt by joining all the batch's stories separated by double empty lines, and adds the 
            # instruction prompts before and after the contents of the batch.
            full_batch_prompt_content = (
                overall_instruction_prefix +
                "\n\n".join(batch_prompt_parts) +
                overall_instruction_suffix
            )

            # Specifies the format of the prompt so that the model is passed things in an order it understands and knows what to process
            # and what its ouput is.
            llama_cli_prompt = (
                "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
                f"{full_batch_prompt_content}"
                "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
            )

            # Specifies what variables need to be passed to what part of the model.
            command = [
                llama_cli_path,
                "-m", model_path,
                "-c", str(max_context_tokens),
                "-n", str(generation_buffer),
                # Controls randomness. Higher leads to higher creativity but higher chance of incoherence. Lower is higher of just taking most
                # probable next token. Adjusted to be lower than other codes as was not generating in the correct format because of too much
                # randomness. Would suggest playing aroung with these numbers to see what works best.
                "--temp", "0.6",
                # Controls nucleus sampling. Here chooses from the most likely of 90% of possible next tokens. Higher gives more variety in
                # words. Again decreased to try get a more consistent output.
                "--top-p", "0.7",
                # Random number generator seed. If same one with same prompt and parameters should always get the same out.
                "--seed", "2",
                "--prompt", llama_cli_prompt,
                "-ngl", "26"
            ]

            # print("\n--- DEBUG INFO: LLAMA-CLI COMMAND ---")
            # print(f"Command array: {command}")
            # # Truncate prompt for debug print to avoid flooding console
            # print(f"Full prompt being sent to llama-cli ({len(llama_cli_prompt)} chars), truncated to first 500 chars:\n{llama_cli_prompt[:500]}...")
            # print("--- END DEBUG INFO ---\n")

            try:
                # Runs the specified command for the model and returns what the model outputs to the standard output.
                result = subprocess.run(command, capture_output=True, text=True, check=True, encoding="utf-8")
                generated_text = result.stdout

                # print("\n--- RAW LLAMA-CLI STDOUT ---")
                # print(generated_text)
                # print("--- END RAW LLAMA-CLI STDOUT ---\n")

                clean_output = ""
                start_of_themes_marker_pattern = re.compile(r"(\*\*Story ID: Story_\d+\*\*.*?)", re.DOTALL | re.IGNORECASE)
                # Finds the first story by finding the first occurence of the Story ID heading.
                match = start_of_themes_marker_pattern.search(generated_text)

                if match:
                    # If the Story ID heading is found, extracts everything from there to the end of output.
                    clean_output = generated_text[match.start():].strip()
                    clean_output = clean_output.replace("[end of text]", "").strip()
                    
                    # If the order of output gets mixed up, removes the possible output informing you that here is what it has generated.
                    intro_sentence_marker_regex = r"^(Here are the stories with the main theme\(sentence\) and theme\(word\) for each:)\s*\n*"
                    clean_output = re.sub(intro_sentence_marker_regex, "", clean_output, 1, re.IGNORECASE | re.MULTILINE).strip()

                else:
                    # If does not find the first Story ID heading, attempts to just process the entire output.
                    print("Critical Error: Could not find the expected start of structured themes ('**Story ID: Story_X**'). Cannot reliably parse. Processing raw output directly (this will likely fail).")
                    clean_output = generated_text.replace("[end of text]", "").strip()

                # print("\n--- DEBUG INFO: CLEAN OUTPUT BEFORE REGEX PARSING ---")
                # print(clean_output)
                # print("--- END DEBUG INFO ---\n")

                # Defines regex for extracting the output we want with Story ID, theme sentence and theme word.
                theme_extraction_pattern = re.compile(
                    r"\*\*Story ID: (Story_\d+)\*\*\s*\n"
                    r"Theme Sentence(?: for Story_\d+)?:?\s*(.*?)(?=\s*Theme Word:|\Z|\n\n\*\*Story ID:|\n\n---\n\n)" 
                    r"\s*Theme Word(?: for Story_\d+)?:?\s*(.*?)(?=\s*\*\*Story ID:|\Z|\n\n---\n\n)",
                    re.DOTALL | re.IGNORECASE
                )
                
                # Looks for all the sections of the form of the specified regex.
                found_themes = theme_extraction_pattern.findall(clean_output)
                
                if not found_themes and clean_output:
                    print("Warning: No themes extracted by the primary regex pattern. Trying a more general fallback pattern.")
                    # Defines a second possibility of extractable themes format.
                    fallback_pattern = re.compile(
                        r"Story ID: (Story_\d+)\s*"
                        r".*?Theme Sentence:?\s*(.*?)\s*"
                        r".*?Theme Word:?\s*(.*?)(?=\s*Story ID:|\Z)",
                        re.DOTALL | re.IGNORECASE
                    )
                    # Looks for all the sections of the form of the secondary regex.
                    found_themes = fallback_pattern.findall(clean_output)

                if found_themes:
                    for story_id, theme_sentence, theme_word in found_themes:
                        # Attempts to ensure the theme word is a word and not more by accident by taking the first line and the first word
                        # of that line..
                        theme_word_clean = theme_word.splitlines()[0].strip()
                        if ' ' in theme_word_clean:
                            print(f"Warning: Theme Word for '{story_id}' contained multiple words: '{theme_word_clean}'. Taking first word.")
                            theme_word_clean = theme_word_clean.split()[0].strip()
                        
                        # Stores the theme sentence and word.
                        processed_themes[story_id] = {
                            "sentence": theme_sentence.strip(),
                            "word": theme_word_clean
                        }
                else:
                    print(f"Could not extract any themes from batch output. Raw output part for debug:\n{clean_output[:500]}...")
                    # If no themes found at all, just says extraction failed for all of them because nothing was of the specified form.
                    for identifier in identifiers_in_batch:
                        processed_themes[identifier] = {
                            "sentence": "Theme extraction failed. (Parsing error - no matches)",
                            "word": "Theme extraction failed. (Parsing error - no matches)"
                        }

            # Catches errors in llama-cli and python.
            except subprocess.CalledProcessError as e:
                print(f"ERROR: llama-cli exited with non-zero status code {e.returncode}")
                print(f"Stdout (if any from llama-cli before error): {e.stdout}")
                print(f"Stderr (llama-cli error messages): {e.stderr}")
                for id_val in identifiers_in_batch:
                    processed_themes[id_val] = {
                        "sentence": f"Error during CLI inference (Exit code {e.returncode}). Check stderr.",
                        "word": f"Error during CLI inference (Exit code {e.returncode}). Check stderr."
                    }
            except Exception as e:
                print(f"    General Python error during batch processing: {e}")
                for id_val in identifiers_in_batch:
                    processed_themes[id_val] = {
                        "sentence": "Error during CLI inference (Python exception).",
                        "word": "Error during CLI inference (Python exception)."
                    }

            # Resets the batch variables.
            current_batch_stories_data = []
            current_batch_token_count = 0

    # For processing the last batch of stories. Repeat of the above.
    if current_batch_stories_data:
        print(f"\nProcessing final batch with {len(current_batch_stories_data)} stories (approx. {current_batch_token_count} content tokens)...")
        batch_prompt_parts = []
        identifiers_in_batch = []
        for id_val, content_val in current_batch_stories_data:
            batch_prompt_parts.append(
                BASE_STORY_PROMPT_STRUCTURE.format(
                    identifier=id_val, story_content=content_val
                )
            )
            identifiers_in_batch.append(id_val)

        full_batch_prompt_content = (
            overall_instruction_prefix +
            "\n\n".join(batch_prompt_parts) +
            overall_instruction_suffix
        )
        llama_cli_prompt = (
            "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
            f"{full_batch_prompt_content}"
            "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        )

        command = [
            llama_cli_path,
            "-m", model_path,
            "-c", str(max_context_tokens),
            "-n", str(generation_buffer),
            "--temp", "0.7",
            "--top-p", "0.9",
            "--seed", "2",
            "--prompt", llama_cli_prompt,
            "-ngl", "26"
        ]

        # print("\n--- DEBUG INFO: FINAL BATCH LLAMA-CLI COMMAND ---")
        # print(f"Command array: {command}")
        # # Truncate prompt for debug print to avoid flooding console
        # print(f"Full prompt being sent to llama-cli ({len(llama_cli_prompt)} chars):\n{llama_cli_prompt[:500]}...")
        # print("--- END DEBUG INFO ---\n")

        try:
            result = subprocess.run(command, capture_output=True, text=True, check=True, encoding="utf-8")
            generated_text = result.stdout

            # print("\n--- RAW LLAMA-CLI STDOUT (FINAL BATCH) ---")
            # print(generated_text)
            # print("--- END RAW LLAMA-CLI STDOUT (FINAL BATCH) ---\n")

            # --- LATEST, MOST ROBUST CLEANING LOGIC (DUPLICATED FOR FINAL BATCH) ---
            clean_output = ""
            start_of_themes_marker_pattern = re.compile(r"(\*\*Story ID: Story_\d+\*\*.*?)", re.DOTALL | re.IGNORECASE)
            
            match = start_of_themes_marker_pattern.search(generated_text)

            if match:
                clean_output = generated_text[match.start():].strip()
                clean_output = clean_output.replace("[end of text]", "").strip()
                
                intro_sentence_marker_regex = r"^(Here are the stories with the main theme\(sentence\) and theme\(word\) for each:)\s*\n*"
                clean_output = re.sub(intro_sentence_marker_regex, "", clean_output, 1, re.IGNORECASE | re.MULTILINE).strip()

            else:
                print("Critical Error: Could not find the expected start of structured themes ('**Story ID: Story_X**') (final batch). Cannot reliably parse. Processing raw output directly (this will likely fail).")
                clean_output = generated_text.replace("[end of text]", "").strip() # Fallback
            # --- END LATEST, MOST ROBUST CLEANING LOGIC (DUPLICATED FOR FINAL BATCH) ---

            print("\n--- DEBUG INFO: CLEAN OUTPUT BEFORE REGEX PARSING (FINAL BATCH) ---")
            print(clean_output)
            print("--- END DEBUG INFO ---\n")

            # Updated Regex to extract themes more robustly for **Story ID: Story_X** format
            theme_extraction_pattern = re.compile(
                r"\*\*Story ID: (Story_\d+)\*\*\s*\n" # Match **Story ID: Story_X** and newline
                r"Theme Sentence(?: for Story_\d+)?:?\s*(.*?)(?=\s*Theme Word:|\Z|\n\n\*\*Story ID:|\n\n---\n\n)" # Capture Theme Sentence
                r"\s*Theme Word(?: for Story_\d+)?:?\s*(.*?)(?=\s*\*\*Story ID:|\Z|\n\n---\n\n)", # Capture Theme Word
                re.DOTALL | re.IGNORECASE
            )
            
            found_themes = theme_extraction_pattern.findall(clean_output)

            if not found_themes and clean_output:
                print("Warning: No themes extracted by the primary regex pattern. Trying a more general fallback pattern.")
                fallback_pattern = re.compile(
                    r"Story ID: (Story_\d+)\s*"
                    r".*?Theme Sentence:?\s*(.*?)\s*"
                    r".*?Theme Word:?\s*(.*?)(?=\s*Story ID:|\Z)",
                    re.DOTALL | re.IGNORECASE
                )
                found_themes = fallback_pattern.findall(clean_output)

            if found_themes:
                for story_id, theme_sentence, theme_word in found_themes:
                    theme_word_clean = theme_word.splitlines()[0].strip() # Take only the first line
                    if ' ' in theme_word_clean:
                        print(f"Warning: Theme Word for '{story_id}' contained multiple words: '{theme_word_clean}'. Taking first word.")
                        theme_word_clean = theme_word_clean.split()[0].strip()

                    processed_themes[story_id] = {
                        "sentence": theme_sentence.strip(),
                        "word": theme_word_clean
                    }
            else:
                print(f"    Could not extract any themes from final batch output. Raw output part for debug:\n{clean_output[:500]}...")
                for identifier in identifiers_in_batch:
                    processed_themes[identifier] = {
                        "sentence": "Theme extraction failed. (Parsing error - no matches)",
                        "word": "Theme extraction failed. (Parsing error - no matches)"
                    }

        except subprocess.CalledProcessError as e:
            print(f"    ERROR: Final batch llama-cli exited with non-zero status code {e.returncode}")
            print(f"    Stdout (if any from llama-cli before error): {e.stdout}")
            print(f"    Stderr (llama-cli error messages): {e.stderr}")
            for id_val in identifiers_in_batch:
                processed_themes[id_val] = {
                    "sentence": f"Error during CLI inference (Exit code {e.returncode}). Check stderr.",
                    "word": f"Error during CLI inference (Exit code {e.returncode}). Check stderr."
                }
        except Exception as e:
            print(f"    General Python error during final batch processing: {e}")
            for id_val in identifiers_in_batch:
                processed_themes[id_val] = {
                    "sentence": "Error during CLI inference (Python exception).",
                    "word": "Error during CLI inference (Python exception)."
                }

    # 4. Append Results to Output File
    print(f"\nAppending results to '{output_file}'...")
    try:
        with open(output_file, "a", encoding="utf-8") as outfile:
            # Using datetime.now() with timezone for South Africa context
            outfile.write(f"\n--- Themes from llama.cpp CLI run on {datetime.now().strftime('%Y-%m-%d %H:%M:%S SAST')} ---\n\n")
            # Iterate and write both sentence and word
            for i in range(len(stories)): # Iterate based on original story count for consistent ordering
                story_identifier = f"Story_{i + 1}"
                themes_data = processed_themes.get(story_identifier, {
                    "sentence": "Theme not found/extracted.",
                    "word": "Theme not found/extracted."
                })
                outfile.write(f"Story: {story_identifier}\n")
                outfile.write(f"Theme Sentence: {themes_data['sentence']}\n")
                outfile.write(f"Theme Word: {themes_data['word']}\n")
                outfile.write("---\n\n")
        print(f"Successfully appended {len(processed_themes)} themes to '{output_file}'.")
    except Exception as e:
        print(f"Error writing to output file: {e}")

# Main method for calling all the above by calling process_stories_with_llama_cli.
if __name__ == "__main__":

    # Checks if the output file is there.
    if not os.path.exists(OUTPUT_FILE_PATH):
        with open(OUTPUT_FILE_PATH, "w", encoding="utf-8") as f:
            f.write("--- Start of Extracted Themes Log ---\n\n")

    process_stories_with_llama_cli(
        input_file=INPUT_FILE_PATH,
        output_file=OUTPUT_FILE_PATH,
        llama_cli_path=LLAMA_CLI_PATH,
        model_path=MODEL_PATH,
        story_delimiter=STORY_DELIMITER,
        max_context_tokens=MAX_CONTEXT_TOKENS,
        generation_buffer=GENERATION_BUFFER
    )

    print(f"\nProcessing complete. Check '{OUTPUT_FILE_PATH}' for results.")
